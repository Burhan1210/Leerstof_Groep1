{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56f197f-f5e1-44c3-a53b-ca98d075ec35",
   "metadata": {},
   "source": [
    "# Object Localisatie\n",
    "\n",
    "In dit notebook gaan we twee kernconcepten van computer vision implementeren: object localisatie en object detectie. We zullen deze oefeningen uitvoeren met behulp van zowel PyTorch als Keras, en we gebruiken zowel zelf te trainen netwerken als pre-trained netwerken om de taken te voltooien. Object localisatie is gericht op het vinden van de positie van een enkel object in een afbeelding, terwijl object detectie het vinden van meerdere objecten met hun bijbehorende klassen omvat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63594cd4-45b3-4ec7-a078-e2ba1ffe1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import opendatasets as od\n",
    "import numpy as np\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/mbkinaci/image-localization-dataset\")\n",
    "\n",
    "# Helper Functies\n",
    "def imshow(img, title=None):\n",
    "    \"\"\"Toont een afbeelding.\"\"\"\n",
    "    npimg = img.numpy() if isinstance(img, torch.Tensor) else img\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def show_image_with_bbox(image, bbox, title=None):\n",
    "    \"\"\"Toont een afbeelding met een bounding box.\"\"\"\n",
    "    npimg = image.numpy() if isinstance(image, torch.Tensor) else image\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    img_size = npimg.shape[1]\n",
    "    xmin = bbox[0] * img_size\n",
    "    ymin = bbox[1] * img_size\n",
    "    width = bbox[2] * img_size\n",
    "    height = bbox[3] * img_size\n",
    "    print(xmin, ymin, width, height)\n",
    "    plt.gca().add_patch(plt.Rectangle((xmin, ymin), width, height,\n",
    "                                      fill=False, edgecolor='red', linewidth=2))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb8e53-8a8f-426a-a707-6f4941032e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_with_bbox(image[0], outputs[0].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433044e9-e9b8-4c46-8beb-dfe668af17e4",
   "metadata": {},
   "source": [
    "## Oefening 1: Object Localisatie met PyTorch\n",
    "\n",
    "In deze oefening gebruiken we PyTorch om een eenvoudig neuraal netwerk te trainen voor object localisatie. Het doel van object localisatie is om de coördinaten van de bounding box rondom een enkel object in een afbeelding te voorspellen. We maken een convolutioneel neuraal netwerk (CNN) dat leert om vier waarden te voorspellen: de x- en y-coördinaten van de bovenste linkerhoek van de bounding box, en de breedte en hoogte van de box. \n",
    "\n",
    "We gebruiken een dummy dataset van willekeurige afbeeldingen en trainen het model om één bounding box te voorspellen voor elke afbeelding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd56c55-5b75-4fa8-bb36-f7d65bdd22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Zet de afbeelding om naar een tensor en normaliseer naar [0, 1]\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# custom dataset aanmaken\n",
    "class XMLDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, img_size=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img_size=img_size\n",
    "\n",
    "        self.images=[f for f in os.listdir(self.root_dir) if f.endswith(\".jpg\")]\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # read image\n",
    "        img_path = os.path.join(self.root_dir, self.images[idx])\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # extract bbox\n",
    "        xml_path = os.path.splitext(img_path)[0] + '.xml' # vervang .jpg door .xml\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot() # parse xml file\n",
    "\n",
    "        width = float(root.find('size/width').text)\n",
    "        height = float(root.find('size/height').text)\n",
    "        \n",
    "        bbox = root.find('object/bndbox')\n",
    "        xmin = float(bbox.find('xmin').text) # haal de waarde uit de node op\n",
    "        xmax = float(bbox.find('xmax').text)\n",
    "        ymin = float(bbox.find('ymin').text)\n",
    "        ymax = float(bbox.find('ymax').text)\n",
    "\n",
    "        # preprocess bbox\n",
    "        # zet het om naar x en y linkerboven hoek en breedte/hoogte van de bbxo\n",
    "        # zet het om naar percentages\n",
    "        bbox = [xmin/width, ymin/height, (xmax-xmin)/width, (ymax-ymin)/height]\n",
    "        \n",
    "        # return image,bbox\n",
    "        return image, torch.tensor(bbox, dtype=torch.float32)\n",
    "\n",
    "dataset = XMLDataset(root_dir='./image-localization-dataset/training_images', transform=transform, img_size=128)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31c1d7-fca5-46d4-a801-f4e8ca97fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    #convolutioneel gedeelte\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1, stride=1), # out_channels kan je kiezen, typisch wordt dit groter als je dieper in het netwerk gaat\n",
    "    nn.ReLU(), # de activatiefunctie!\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1), # in_channels gelijk aan out_channels vorige\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=1), \n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1), \n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1), \n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    #Flatten (maak er 1d van van elk figuur)\n",
    "    nn.Flatten(),\n",
    "    #Fully connected gedeelte\n",
    "    nn.Linear(16*64, 128),  # 16384 te berekenen door het model eens uit te voeren tot de flatten of 16*16*64\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 4)\n",
    ")\n",
    "\n",
    "for image,target in dataloader:\n",
    "    print(model(image).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44b854-2c6f-4091-8515-367edc4652bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for image,target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss+= loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}: running loss is {epoch_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ffa9f0-ee0a-420e-a5fd-0759632d77a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images,targets = next(iter(dataloader))\n",
    "outputs = model(images)\n",
    "\n",
    "print(targets[0])\n",
    "print(outputs[0])\n",
    "\n",
    "show_image_with_bbox(image[1], targets[1])\n",
    "show_image_with_bbox(image[1], outputs[1].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c01bb4-3add-4a29-8e68-1e591dc63f09",
   "metadata": {},
   "source": [
    "## Oefening 2: Object Detectie met PyTorch\n",
    "\n",
    "In deze oefening gebruiken we PyTorch om object detectie uit te voeren met een pre-trained Faster R-CNN-model. Faster R-CNN (Region-Based Convolutional Neural Network) is een state-of-the-art model voor object detectie dat zowel de bounding boxes als de klassen van meerdere objecten in een afbeelding kan voorspellen.\n",
    "\n",
    "We maken gebruik van een pre-trained Faster R-CNN-model dat beschikbaar is via de `torchvision` bibliotheek. Dit model is al getraind op de COCO dataset, dus we kunnen het gebruiken om objecten in nieuwe afbeeldingen te detecteren zonder aanvullende training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97a0b9-5d1c-47b2-ac25-e4e02898729d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "# Define helper function to display image with bounding boxes\n",
    "def show_image_with_bboxes(image, bboxes, title=None):\n",
    "    \"\"\"Displays an image with bounding boxes.\"\"\"\n",
    "    plt.imshow(F.to_pil_image(image))  # Convert tensor image to PIL image for displaying\n",
    "    ax = plt.gca()  # Get the current axes instance on the current figure\n",
    "    \n",
    "    # Draw all bounding boxes\n",
    "    for bbox in bboxes.detach():\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                             fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Define the transform to convert the image to a tensor\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the single image\n",
    "image_path = 'object-detection/img1.jpg'  # Path to your image file\n",
    "image = Image.open(image_path).convert('RGB')  # Open the image file and ensure it's in RGB mode\n",
    "\n",
    "# Apply the transformation\n",
    "image_tensor = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "\n",
    "# LOAD THE MODEL\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.7)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875df61-c4b6-4ece-bc33-56b3743e872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "prediction = model(image_tensor)[0]\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "box = draw_bounding_boxes((image_tensor*255).type(torch.uint8)[0], boxes=prediction[\"boxes\"],\n",
    "                          labels=labels,\n",
    "                          colors=\"red\",\n",
    "                          width=4, font_size=30)\n",
    "im = to_pil_image(box.detach())\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4d5c5-4b32-478d-85e4-545fc889275b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ca55f3f-70f5-4322-847f-0ded9ccca5e6",
   "metadata": {},
   "source": [
    "## Oefening 3: Object Localisatie met Keras\n",
    "\n",
    "In deze oefening implementeren we object localisatie met Keras. Net als in Oefening 1 bouwen we een eenvoudig CNN-model dat leert om de coördinaten van een bounding box te voorspellen voor een enkel object in een afbeelding. Dit model wordt getraind op een dummy dataset met willekeurige afbeeldingen en dummy bounding box-coördinaten.\n",
    "\n",
    "Het model bestaat uit een reeks convolutie- en poolinglagen, gevolgd door volledig verbonden lagen die de bounding box-coördinaten voorspellen. We gebruiken Mean Squared Error (MSE) als de verliesfunctie om de coördinaten te trainen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afddba7-7439-4e2c-ac52-ff238d9768b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c10b96-59d4-43c9-8814-2a213945c19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5c403-c203-490f-9bbc-9bfda7dda229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
